## 思考题

### 1、Residual learning 的基本原理？

残差学习的基本原理是通过在神经网络中引入跳跃连接，为每个残差块的输出与输入之间建立了联系，使网络能够学习输入与输出之间的残差映射而非直接映射，并有效解决了深层网络的梯度消失/爆炸的问题。

### 2、Batch Normailization 的原理，思考 BN、LN、IN 的主要区别。

在深层神经网络中，层与层之间的输入分布会随着参数更新不断发生变化，这种现象被称为内部协变量偏移。具体来说，由于网络层逐层堆叠，各层参数的更新可能会导致前一层的输出分布发生变化，进而影响到下一层的输入分布。Batch Normalization（BN）的核心原理是通过标准化神经网络的中间层输入分布（即 ‌对每个特征通道独立计算批量内均值和方差‌），解决深层网络训练中的内部协变量偏移问题。

BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。

LN在通道方向上，对C、H、W归一化。

IN在图像像素上，对H、W做归一化

### 3、为什么分组卷积可以提升准确率？既然分组卷积可以提升准确率，同时还能降低计算量，分组数量尽量多不⾏吗？

分组卷积提升准确率的原因有：
1.将输入通道分组后，每组卷积核独立学习‌互补的特征子空间‌，避免传统卷积中所有通道被迫学习相似模式造成的特征冗余。
2.分组降低了模型自由度，迫使网络学习更具判别性的局部特征，显著抑制过拟合。

分组数量不能尽量多的原因：分组数量越多，会使特征割裂严重，导致通道间语义关联断裂，反而降低精确度。当分组数等于通道数时，退化为深度可分离卷积。

